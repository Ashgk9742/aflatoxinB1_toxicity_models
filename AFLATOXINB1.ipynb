{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a list of differentially expressed genes (DEGs) from your GEO2R dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing GEO2R files\n",
    "input_folder = \"path/to/filename/datasets for aflatoxinb1\"  # Replace with the actual folder path containing GEO2R files\n",
    "output_file = \"path/to/processed_/Compiled_GEO2R.csv\"  # Replace with the desired output file path\n",
    "\n",
    "# Define the required columns\n",
    "required_columns = ['ID', 'adj.P.Val', 'P.Value','Gene.symbol']\n",
    "\n",
    "# Initialize an empty DataFrame to store compiled data\n",
    "compiled_data = pd.DataFrame(columns=required_columns)\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.tsv'):  # Process only TSV files\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Read the GEO2R file\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, sep='\\t')\n",
    "            \n",
    "            # Select only the required columns (if they exist in the file)\n",
    "            data = data[required_columns]\n",
    "            \n",
    "            # Append the data to the compiled DataFrame\n",
    "            compiled_data = pd.concat([compiled_data, data], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Save the compiled data to a CSV file\n",
    "compiled_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Compiled data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CTD dataset\n",
    "file_path = 'path/to/CTD_D016604_diseases_20241212062811.tsv'\n",
    "ctd_data = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# Step 1: Split the 'Inference Network' column into individual gene symbols\n",
    "# Explode the gene list into separate rows\n",
    "ctd_data['Inference Network'] = ctd_data['Inference Network'].str.split('|')\n",
    "exploded_ctd_data = ctd_data.explode('Inference Network')\n",
    "\n",
    "# Step 2: Standardize gene symbols\n",
    "# Assuming GEO2R uses uppercase symbols, convert all gene symbols to uppercase\n",
    "exploded_ctd_data['Inference Network'] = exploded_ctd_data['Inference Network'].str.upper()\n",
    "\n",
    "# Step 3: Filter relevant columns for further analysis\n",
    "# Keeping only necessary columns\n",
    "filtered_ctd_data = exploded_ctd_data[['Chemical Name', 'Disease Name', 'Inference Network', 'Inference Score']]\n",
    "\n",
    "# Step 4: Normalize the 'Inference Score'\n",
    "# Min-Max scaling of the 'Inference Score'\n",
    "min_score = filtered_ctd_data['Inference Score'].min()\n",
    "max_score = filtered_ctd_data['Inference Score'].max()\n",
    "filtered_ctd_data['Normalized Inference Score'] = (filtered_ctd_data['Inference Score'] - min_score) / (max_score - min_score)\n",
    "\n",
    "# Step 5: Save the preprocessed data for comparison with GEO2R\n",
    "output_file_path = '/Preprocessed_CTD_Data.csv'\n",
    "filtered_ctd_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to: {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ctd_data.loc[:, 'Normalized Inference Score'] = (\n",
    "    filtered_ctd_data['Inference Score'] - min_score\n",
    ") / (max_score - min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace with the actual path to your preprocessed CTD data\n",
    "ctd_file_path = \"path/to/Preprocessed_CTD_Data1.csv\"\n",
    "ctd_data = pd.read_csv(ctd_file_path)\n",
    "\n",
    "print(\"CTD data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your GEO2R DEGs file\n",
    "geo2r_file_path = \"path/to/Compiled_GEO2R.csv\"\n",
    "geo2r_data = pd.read_csv(geo2r_file_path)\n",
    "\n",
    "print(\"GEO2R DEGs data loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = \"path/to/Preprocessed_CTD_Data.csv\"  # Replace with the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Rename the column\n",
    "data.rename(columns={\"Inference Network\": \"Gene.symbol\"}, inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to the CSV\n",
    "output_file_path =\"path/to/Preprocessed_CTD_Data1.csv\"  # Replace with the desired output path\n",
    "data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Column renamed and data saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load GEO2R and CTD datasets\n",
    "geo2r_path = \"path/to/Compiled_GEO2R.csv\"  # Replace with your GEO2R file path\n",
    "ctd_path = \"path/to/Preprocessed_CTD_Data1.csv\"      # Replace with your CTD file path\n",
    "\n",
    "geo2r_data = pd.read_csv(geo2r_path)\n",
    "ctd_data = pd.read_csv(ctd_path)\n",
    "\n",
    "# Step 2: Inspect the datasets (Optional)\n",
    "print(\"GEO2R Dataset Head:\")\n",
    "print(geo2r_data.head())\n",
    "print(\"\\nCTD Dataset Head:\")\n",
    "print(ctd_data.head())\n",
    "\n",
    "# Step 3: Rename relevant columns for consistency\n",
    "ctd_data.rename(columns={\"Inference Network\": \"Gene.symbol\"}, inplace=True)\n",
    "\n",
    "# Step 4: Merge datasets (Outer Join on 'Gene.symbol')\n",
    "merged_data = pd.merge(geo2r_data, ctd_data, on=\"Gene.symbol\", how=\"outer\")\n",
    "\n",
    "# Step 5: Save merged dataset\n",
    "output_file_path = \"path_to_output_merged_file.csv\"  # Replace with your desired output path\n",
    "merged_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Merged data saved to: {output_file_path}\")\n",
    "\n",
    "# Step 6: Analyze the merged dataset\n",
    "# Example Analysis: Check for overlapping genes\n",
    "geo2r_genes = set(geo2r_data['Gene.symbol'].dropna())\n",
    "ctd_genes = set(ctd_data['Gene.symbol'].dropna())\n",
    "\n",
    "overlapping_genes = geo2r_genes.intersection(ctd_genes)\n",
    "unique_to_geo2r = geo2r_genes - ctd_genes\n",
    "unique_to_ctd = ctd_genes - geo2r_genes\n",
    "\n",
    "print(f\"\\nNumber of overlapping genes: {len(overlapping_genes)}\")\n",
    "print(f\"Number of unique genes in GEO2R: {len(unique_to_geo2r)}\")\n",
    "print(f\"Number of unique genes in CTD: {len(unique_to_ctd)}\")\n",
    "\n",
    "# Example Visualization (Optional, Requires matplotlib and seaborn)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot overlapping genes\n",
    "venn_labels = {\n",
    "    \"unique_to_geo2r\": len(unique_to_geo2r),\n",
    "    \"unique_to_ctd\": len(unique_to_ctd),\n",
    "    \"overlapping_genes\": len(overlapping_genes),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.bar(venn_labels.keys(), venn_labels.values(), color=['blue', 'orange', 'green'])\n",
    "plt.xlabel(\"Gene Overlap Categories\")\n",
    "plt.ylabel(\"Number of Genes\")\n",
    "plt.title(\"Overlap Between GEO2R and CTD Datasets\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for combined datasets CTD data 1 and CTD data 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "file_1_path = 'path/to/cleaned_data.csv'  # Update to the actual path\n",
    "file_2_path = 'path/to/CTD_D016604_ixns_20241212063652.csv'  # Update to the actual path\n",
    "\n",
    "data_1 = pd.read_csv(file_1_path)\n",
    "data_2 = pd.read_csv(file_2_path)\n",
    "\n",
    "# Ensure column names match for merging\n",
    "data_2.rename(columns={'Gene Symbol': 'Gene.symbol', 'Chemical Name': 'Chemical Name'}, inplace=True)\n",
    "\n",
    "# Perform an inner join on common columns: 'Gene.symbol' and 'Chemical Name'\n",
    "combined_data = pd.merge(data_1, data_2, on=['Gene.symbol', 'Chemical Name'], how='inner')\n",
    "\n",
    "# Save the combined dataset to a file\n",
    "combined_file_path = 'path/to/combined_dataafl.csv'  # Update to the desired save path\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "# Display some details about the combined dataset\n",
    "print(f\"Combined data has {combined_data.shape[0]} rows and {combined_data.shape[1]} columns.\")\n",
    "print(f\"Combined dataset saved to: {combined_file_path}\")\n",
    "print(combined_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = ['Chemical Name', 'Gene.symbol', 'Disease Name', 'Interaction Actions']\n",
    "numerical_columns = ['adj.P.Val', 'P.Value', 'Inference Score', 'Normalized Inference Score', 'Reference Count', 'Organism Count']\n",
    "\n",
    "# Initialize sparse one-hot encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_sparse = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Combine sparse encoded features with numerical features\n",
    "X_sparse = hstack([encoded_sparse, data[numerical_columns]])\n",
    "\n",
    "# Convert the target variable into numerical format\n",
    "y = data['Toxicity_Level'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_sparse, X_temp_sparse, y_train, y_temp = train_test_split(\n",
    "    X_sparse, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "X_val_sparse, X_test_sparse, y_val, y_test = train_test_split(\n",
    "    X_temp_sparse, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Save the sparse datasets to files\n",
    "import joblib\n",
    "joblib.dump(X_train_sparse,'path/to/X_train_sparse.pkl')\n",
    "joblib.dump(X_val_sparse,'path/to/X_val_sparse.pkl')\n",
    "joblib.dump(X_test_sparse,'path/to/X_test_sparse.pkl')\n",
    "joblib.dump(y_train,'path/to/y_train.pkl')\n",
    "joblib.dump(y_val,'path/to/y_val.pkl')\n",
    "joblib.dump(y_test,'path/to/y_test.pkl')\n",
    "\n",
    "print(\"Sparse datasets saved successfully:\")\n",
    "print(\"- Training set: X_train_sparse.pkl, y_train.pkl\")\n",
    "print(\"- Validation set: X_val_sparse.pkl, y_val.pkl\")\n",
    "print(\"- Test set: X_test_sparse.pkl, y_test.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = model.predict(X_val_sparse)\n",
    "\n",
    "# Validation metrics\n",
    "print(\"Validation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['Low', 'Medium', 'High']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = model.predict(X_test_sparse)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low', 'Medium', 'High']))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'path/to/logistic_regression_model_imputed.pkl')\n",
    "print(\"Model saved successfully: logistic_regression_model_imputed.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_sparse, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"5-Fold Cross-Validation Accuracy Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NNeighbors Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "# Train k-NN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate k-NN\n",
    "y_val_pred_knn = knn_model.predict(X_val_sparse)\n",
    "print(\"k-NN - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_knn))\n",
    "print(confusion_matrix(y_val, y_val_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Perform Grid Search\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "knn_grid.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Best k-value\n",
    "print(f\"Best k-value: {knn_grid.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize k-NN with the best k-value\n",
    "knn_best = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train k-NN\n",
    "knn_best.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred_knn_best = knn_best.predict(X_val_sparse)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"k-NN (k=3) - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_knn_best))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_knn_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the preprocessed test feature data (X_test_sparse)\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "\n",
    "# Load the test target labels (y_test)\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "print(\"Test data loaded successfully!\")\n",
    "print(f\"X_test_sparse shape: {X_test_sparse.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Initialize the imputer (mean strategy)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Impute missing values in sparse matrix\n",
    "X_test_sparse_imputed = imputer.fit_transform(X_test_sparse)\n",
    "\n",
    "# Ensure it's still in sparse format\n",
    "X_test_sparse_imputed = csr_matrix(X_test_sparse_imputed)\n",
    "\n",
    "print(\"Missing values imputed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Convert sparse matrix to dense to handle NaNs\n",
    "X_test_dense = X_test_sparse.toarray()\n",
    "\n",
    "# Identify rows without NaNs\n",
    "non_nan_indices = ~np.isnan(X_test_dense).any(axis=1)\n",
    "\n",
    "# Filter out rows with NaNs and convert back to sparse format\n",
    "X_test_sparse_cleaned = csr_matrix(X_test_dense[non_nan_indices])\n",
    "\n",
    "# Ensure corresponding labels are also filtered\n",
    "y_test_cleaned = y_test[non_nan_indices]\n",
    "\n",
    "print(f\"Rows with NaNs removed. Remaining rows: {X_test_sparse_cleaned.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using cleaned data\n",
    "y_test_pred_knn = knn_model.predict(X_test_sparse_cleaned)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"k-NN (k=3) - Test Performance (After Removing NaNs)\")\n",
    "print(classification_report(y_test_cleaned, y_test_pred_knn))\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cleaned, y_test_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the optimized k-NN model\n",
    "joblib.dump(knn_best,'path/to/knn_best_model.pkl')\n",
    "print(\"Optimized k-NN model saved successfully: knn_best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate XGBoost\n",
    "y_val_pred_xgb = xgb_model.predict(X_val_sparse)\n",
    "\n",
    "# Print results\n",
    "print(\"XGBoost - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_xgb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_xgb = xgb_model.predict(X_test_sparse)\n",
    "\n",
    "print(\"XGBoost - Test Performance\")\n",
    "print(classification_report(y_test, y_test_pred_xgb))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb_model,'path/to/xgboost_best_model.pkl')\n",
    "\n",
    "print(\"XGBoost model saved successfully: xgboost_best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "\n",
    "# Initialize and train LightGBM\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "lgbm_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "y_val_pred_lgbm = lgbm_model.predict(X_val_sparse)\n",
    "\n",
    "# Print results\n",
    "print(\"LightGBM - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_lgbm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lgbm = lgbm_model.predict(X_test_sparse)\n",
    "\n",
    "print(\"LightGBM - Test Performance\")\n",
    "print(classification_report(y_test, y_test_pred_lgbm))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_lgbm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained LightGBM model\n",
    "joblib.dump(lgbm_model,'path/to/lightgbm_best_model.pkl')\n",
    "\n",
    "print(\"LightGBM model saved successfully: lightgbm_best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "# Initialize and train Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "y_val_pred_rf = rf_model.predict(X_val_sparse)\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_rf = rf_model.predict(X_test_sparse)\n",
    "\n",
    "print(\"Random Forest - Test Performance\")\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf_model,'path/to/random_forest_best_model.pkl')\n",
    "\n",
    "print(\"Random Forest model saved successfully: random_forest_best_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "# Load the sparse datasets\n",
    "X_train_sparse = joblib.load('path/to/X_train_sparse.pkl')\n",
    "X_val_sparse = joblib.load('path/to/X_val_sparse.pkl')\n",
    "X_test_sparse = joblib.load('path/to/X_test_sparse.pkl')\n",
    "y_train = joblib.load('path/to/y_train.pkl')\n",
    "y_val = joblib.load('path/to/y_val.pkl')\n",
    "y_test = joblib.load('path/to/y_test.pkl')\n",
    "\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_sparse = imputer.fit_transform(X_train_sparse)\n",
    "X_val_sparse = imputer.transform(X_val_sparse)\n",
    "X_test_sparse = imputer.transform(X_test_sparse)\n",
    "\n",
    "# Train CatBoost\n",
    "cat_model = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.1, verbose=100, random_state=42)\n",
    "cat_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_val_pred_cat = cat_model.predict(X_val_sparse)\n",
    "\n",
    "print(\"CatBoost - Validation Performance\")\n",
    "print(classification_report(y_val, y_val_pred_cat))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_cat = cat_model.predict(X_test_sparse)\n",
    "\n",
    "print(\"CatBoost - Test Performance\")\n",
    "print(classification_report(y_test, y_test_pred_cat))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained CatBoost model\n",
    "joblib.dump(cat_model,'path/to/catboost_best_model.pkl')\n",
    "\n",
    "print(\"CatBoost model saved successfully: catboost_best_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
